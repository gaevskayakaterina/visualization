# -*- coding: utf-8 -*-
"""09_string_2_Фамилия_Имя.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12lzVg3_-x6jHt1XkkFYn69wMEkXreA5E

# Введение в обработку текста на естественном языке

Материалы:
* Макрушин С.В. Лекция 9: Введение в обработку текста на естественном языке\
* https://realpython.com/nltk-nlp-python/
* https://scikit-learn.org/stable/modules/feature_extraction.html

## Задачи для совместного разбора
"""

from sklearn.feature_extraction.text import CountVectorizer
import pymorphy2

"""1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. """

text = '''с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'''

"""2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов.

3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`.

## Лабораторная работа 9

### Расстояние редактирования

1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`).
"""

import nltk
nltk.download('punkt')
import nltk.tokenize
from nltk.tokenize import word_tokenize
import pandas as pd
data = pd.read_csv('preprocessed_descriptions.csv')
#data
#a = data['preprocessed_descriptions'][:3]
#word_tokenize(data.preprocessed_descriptions[1])

wordsi = set()
for i in range (len(data)):
  words = []
  if type(data.preprocessed_descriptions[i]) == str:
    words.append(word_tokenize(data.preprocessed_descriptions[i]))
  for g in range(len(words)):
    wordsi.update(words[g])
     
wordsie = list(wordsi)
print(wordsie[:12])

len(wordsie)

data[:12]

"""1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."""

from nltk.metrics.distance import edit_distance
import nltk
nltk.download('punkt')
import nltk.tokenize
from nltk.tokenize import word_tokenize
import pandas as pd
import random
for i in range(5):
  a =  random.randint(0,len(wordsie))
  b = random.randint(0,len(wordsie))
  dis = edit_distance(wordsie[a],wordsie[b])
  print(f"Расстояние между'{wordsie[a]}'и '{wordsie[b]}'-'{dis}' шагов")

"""1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"""

clovo = int(input('Введите индекс слова от 0 до 32867: '))
bliz = int(input('Сколько ближайших слов вы хотите получить?' ))
import random
mini = {}
mini2 = {}
for i in range(1,bliz+1):
  d = edit_distance(wordsie[clovo],wordsie[clovo+i])
  b = edit_distance(wordsie[clovo],wordsie[clovo-i])
  #mini.append(wordsie[clovo+i])
  #mini.append(d)
  #mini.append(wordsie[clovo-i])
  #mini.append(b)
  mini[d] = wordsie[clovo+i]
  mini2[b] = wordsie[clovo-i]
  
#mini.sort()
  
  #print(wordsie[clovo],'and',wordsie[clovo + i] ,mini[0])
print(wordsie[clovo-1],wordsie[clovo],wordsie[clovo+1])
print(mini)
print(mini2)

def fun(word,k):
  mini = {}
  for i in wordsie:
    mini[i] = edit_distance(i,word)
  sorte = sorted(mini.items(),key = lambda x: x[1])
  for i in dict(sorte[:k]).keys():
    print(i)
fun('goat', 11)

"""### Стемминг, лемматизация

2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: 
    * word
    * stemmed_word 
    * normalized_word 

Столбец `word` укажите в качестве индекса. 

Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации.
"""

from numpy.lib.arraysetops import setxor1d
from nltk.stem import SnowballStemmer
import pandas as pd
from nltk.stem import WordNetLemmatizer
stemming = SnowballStemmer('english')
stemming.stem('goats')
lize = WordNetLemmatizer()
#lize.lemmatize("rocks")
s1 = []
m = []
for i in wordsie:
  a = stemming.stem(i)
  s1.append(a)
newwordsie = pd.DataFrame({'stemmed_word': s1})
newwordsie.index = wordsie
newwordsie

"""2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."""

nltk.download("stopwords")
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
stop_words = set(stopwords.words("english"))
filtered_list = []
for i in wordsie:
  if i not in stop_words:
    filtered_list.append(i)
print(len(wordsie))
print(len(filtered_list))
print('Стоп-слова составляют: ')
print((len(wordsie)-len(filtered_list))/len(wordsie))



"""### Векторное представление текста

3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`
"""

import random
from random import randint
s = []
data = pd.read_csv('preprocessed_descriptions.csv')
for i in range(5):
  a =  random.randint(0,len(data.name))
  s.append(data.preprocessed_descriptions[a])
  #s.append(a)
print(s)

from sklearn.feature_extraction.text import (TfidfVectorizer,CountVectorizer)
vectorizer = TfidfVectorizer()
cor = vectorizer.fit_transform(s)
cor.toarray()

"""3.2 Вычислите близость между каждой парой рецептов, выбранных в задании 3.1, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."""



"""3.3 Какие рецепты являются наиболее похожими? Прокомментируйте результат (словами)."""

